{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-02\n",
    "## Probability Model A First Look: An Introduction of Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Review the course online programming code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to `Lecture-02.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Review the main points of this lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. How to Github and Why do we use Jupyter and Pycharm; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Github** most commonly used commands:\n",
    "```bash\n",
    "git init\n",
    "git add [file_name]\n",
    "git commit -m \"commit message\"\n",
    "git push origin [branch name] \n",
    "```\n",
    "[Cheat sheat](https://education.github.com/git-cheat-sheet-education.pdf)\n",
    "\n",
    "**Jupyter Notebook**  \n",
    "Used for writing scripts. It's nice for presenting code and conveying information/visualization. It's also suitable for iterating ideas.\n",
    "\n",
    "**Pycharm**  \n",
    "full-fledged IDE, used for building projects. It integrates debugger, CVS, code profiliing and tons of other features to help with development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A probability model is a mathematical representation of a random phenomenon. It is defined by its sample space, events within the sample space, and probabilities associated with each event. The sample space S for a probability model is the set of all possible outcomes.  \n",
    "reference: [probability models](http://www.stat.yale.edu/Courses/1997-98/101/probint.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Can you came up with some scenarios at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* language model\n",
    "* weather prediction\n",
    "* gambling\n",
    "* disease diagnosis based on symptoms\n",
    "* sales forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many real-world problems are probability problems in natural, in that case, probability models can represent these problems more easily and more precisely than rule-based models.  \n",
    "  \n",
    "difficulties:\n",
    "* complex for defining the patterns and programming, especially when the application scenario is complicated.\n",
    "* not flexible  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (probabilistic) language model is a probability distribution over sequences of words. Given such a sequence, say of length $m$, it assigns a probability $P(w_1, w_2, \\ldots, w_m)$ to the whole sequence.  \n",
    "*elaboration:* Assume that the probability of a word only depends on the previous n words. This is known as an `n-gram` model or unigram model when n = 1. The unigram model is also known as the `bag-of-words` model.  \n",
    "reference: [Language Model](https://en.wikipedia.org/wiki/Language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  6. Can you came up with some sceneraies at which we could use Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* speech recognition\n",
    "* Optical Character Recognition (OCR)\n",
    "* text understanding\n",
    "* machine translation\n",
    "* information retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the probability of each word only depends on that word's own probability in the document, so we only have one-state finite automata as units. the probability distribution over the entire vocabulary of the model sums to 1. In mathematical terms: \n",
    "$$P(w_1, w_2, \\ldots, w_m) = P(w_1)P(w_2) \\ldots P(w_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "* simple and effective  \n",
    "  \n",
    "Disadvantages:\n",
    "* sparsity representation may have scalability problem\n",
    "* ignores the context of words, lose the order information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.  What't the 2-gram models; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the probability of observing the $i$th word $w_i$ in the context history of the preceding $i âˆ’ 1$ words can be approximated by the probability of observing it in the shortened context history of the preceding one word. In mathematical terms:\n",
    "$$P(w_1, w_2, \\ldots, w_m) = P(w_1)P(w_2 | w_1) \\ldots P(w_m | w_{m-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. what's the web crawler, and can you implement a simple crawler? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a program or automated script that systematically collects content from the web. To be more specific, it starts from one seed page, extract the links on that page, follow those links to other pages, and keep going.   \n",
    "**A simple crawler**:\n",
    "1. get the response from a url in a list of urls to crawl. `response = requests.get(url)`\n",
    "2. extract information we need from the url. `re.findall(regex_pattern, response.text)`\n",
    "3. update the list of urls to crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.  There may be some issues to make our crwaler programming difficult, what are these, and how do we solve them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. JavaScript website\n",
    "*solution:*   \n",
    "In general, the crawler needs to pretend to be a browser, let all the content load, and only then go and get the HTML to parse. We may use `Selenium`(make the crawler interact with a website just as a human would do) or `WebKit`(open source web browser engine) to crawl JavaScript website.\n",
    "2. anti-crawler, ban accesses from a particular IP or user id.  \n",
    "*suggestion:* \n",
    "Be nice and follow a website's crawling policies. Make the crawling slower, disguise the requests by rotating IPs and proxy services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. What is Regular Expression and how to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regular expression is a sequence of characters that define a search pattern. \n",
    "reference: [Regular Expression](https://en.wikipedia.org/wiki/Regular_expression)  \n",
    "Regular expressions are widely used in text processing tasks for text matching, e.g., web crawling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using Wikipedia dataset to finish the language model. \n",
    "English corpus: https://dumps.wikimedia.org/enwiki/20190320/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create (part of) English Wikipedia Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(path, outfile):\n",
    "    \"\"\"Concatenate the plain texts extracted by WikiExtractor and save as a .csv file.\"\"\"\n",
    "    text = []\n",
    "    for d in glob.glob(enwiki_path):\n",
    "        for file in os.listdir(d):\n",
    "            # DecodeError: using `errors='ignore` will strip out \n",
    "            # some characters not encoded as 'utf-8'\n",
    "            with open(os.path.join(d, file), 'r', errors='ignore') as f:\n",
    "                text += f.readlines()\n",
    "    df = pd.DataFrame([line for line in text if line.strip()], columns=[\"text\"])\n",
    "    df.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwiki_path = \"data/enwiki/*\"\n",
    "outfile = \"data/enwiki_corpus.csv\"\n",
    "create_corpus(enwiki_path, outifle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/enwiki_corpus.csv\")\n",
    "text = df[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tag(line):\n",
    "    \"\"\"Remove html tags in a string.\"\"\"\n",
    "    pattern = re.compile(\"<[^>]*>\")\n",
    "    return re.sub(pattern, \"\", line)\n",
    "\n",
    "def remove_punctuation(line):\n",
    "    \"\"\"Remove special characters in a string.\"\"\"\n",
    "    pattern = re.compile(\"[^A-Za-z0-9- ]\")\n",
    "    return re.sub(pattern, \"\", line)\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Trim whitespaces, remove html tags and special characters of a list of strings.\"\"\"\n",
    "    return [remove_punctuation(remove_html_tag(line.strip().lower())) \n",
    "            for line in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    \"\"\"Get all tokens in a list of strings.\"\"\"\n",
    "    return [word\n",
    "            for line in cleaned_text if line.strip()\n",
    "            for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = preprocess(text)\n",
    "VOCABULARY = get_tokens(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram_counts(words, n):\n",
    "    \"\"\"Calculate the frequency of n-gram.\"\"\"\n",
    "    n_gram_phrases = [' '.join(words[i:i+n]) for i in range(len(words)-(n-1))]\n",
    "    n_gram_count = Counter(n_gram_phrases)\n",
    "    n_gram_total = len(n_gram_phrases)\n",
    "    return (n_gram_count, n_gram_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prob(word):\n",
    "#     \"\"\"Calculate the probability of a single word.\"\"\"\n",
    "#     if word in words_count:\n",
    "#         return words_count[word] / words_total\n",
    "#     else:\n",
    "#         return 1. / words_total # deal with OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joint_prob(n, *args):\n",
    "    \"\"\"Calculate joint probability of all args.\"\"\"\n",
    "    count = eval('_{}_gram_count'.format(n))\n",
    "    total = eval('_{}_gram_total'.format(n))\n",
    "    _n_gram = args[0]\n",
    "    for w in args[1:]:\n",
    "        _n_gram += ' ' + w\n",
    "    # _2_gram = w1 + ' ' + w2\n",
    "    if _n_gram in count:\n",
    "        return count[_n_gram] / total\n",
    "    else:\n",
    "        return 1. / total\n",
    "    \n",
    "def get_conditional_prob(w1, w2):\n",
    "    \"\"\"Calculate conditional probability P(w2|w1).\"\"\"\n",
    "    return get_joint_prob(2, w1, w2) / get_joint_prob(1, w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_1_gram_count, _1_gram_total = get_n_gram_counts(VOCABULARY, 1)\n",
    "# unigram_count = Counter(VOCABULARY)\n",
    "# unigram_total = len(VOCABULARY) # sum([f for w, f in words_count.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product(nums):\n",
    "    \"\"\"Calculate the product of all numbers.\"\"\"\n",
    "    return np.prod(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_model(sen):\n",
    "    \"\"\"Return the probability of the sentence using unigram language model.\"\"\"\n",
    "    words = sen.strip().lower().split()\n",
    "    return product([get_joint_prob(1, w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.975108238625902e-17"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model(\"Today is a sunny day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.819117218767523e-14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model(\"tomorrow will rain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2_gram_count, _2_gram_total = get_n_gram_counts(VOCABULARY, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_gram_model(sen):\n",
    "    \"\"\"Return the probability of the sentence using 2-gram language model.\"\"\"\n",
    "    prob = 1.\n",
    "    words = sen.strip().lower().split()\n",
    "    for i, w in enumerate(words):\n",
    "        if i == 0:\n",
    "            prob *= get_joint_prob(1, w)\n",
    "        else:\n",
    "            prob *= get_conditional_prob(words[i-1], w)\n",
    "            \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.815572708081116e-10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model(\"how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_gram_model(\"how do you do\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_3_gram_count, _3_gram_total = get_n_gram_counts(VOCABULARY, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_gram_model(sen):\n",
    "    \"\"\"Return the probability of the sentence using 3-gram language model.\"\"\"\n",
    "    prob = 1.\n",
    "    words = sen.strip().lower().split()\n",
    "    for i, w in enumerate(words):\n",
    "        if i == 0:\n",
    "            prob *= get_joint_prob(1, w)\n",
    "        elif i == 1:\n",
    "            prob *= get_conditional_prob(words[i-1], w)\n",
    "        else:\n",
    "            prob *= (get_joint_prob(3, words[i-2], words[i-1], w) / \n",
    "                     get_conditional_prob(words[i-2], words[i-1]))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2015208128683736e-19"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_gram_model(\"Today is a beautiful day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Try some interested sentence pairs, and check if your model could fit them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning I shot an elephant  is more possible\n",
      "---- One morning I shot an elephant  with probility 1.0977121145145774e-18\n",
      "----  One morning I eat an elephant with probility 4.142264610773665e-19\n",
      " I went to Antarctica last Month is more possible\n",
      "---- I went to China last Month  with probility 3.411796448061578e-17\n",
      "----  I went to Antarctica last Month with probility 5.4131492098626475e-17\n",
      "The computer is running  is more possible\n",
      "---- The computer is running  with probility 2.4745655473949806e-11\n",
      "----  The computer is walking with probility 2.986544626166356e-12\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    \"One morning I shot an elephant & One morning I eat an elephant\",\n",
    "    \"I went to China last Month & I went to Antarctica last Month\",\n",
    "    \"The computer is running & The computer is walking\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split('&')\n",
    "    p1, p2 = two_gram_model(s1), two_gram_model(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model fits the first and the third sentence pairs, but could not fit the second sentence pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. If we need to solve following problems, how can language model help us? \n",
    "\n",
    "+ Speech Recognition.  \n",
    "+ Sogou *pinyin* input.\n",
    "+ Auto correction in search engine. \n",
    "+ Abnormal Detection.\n",
    "\n",
    "Calculate the probability of a particular word sequence(input) using a language model, if the probability is abnormaly small, then something might be something wrong with the input and the language model can correct it with more probable word sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to the previous learned parsing and pattern match problems. What's the advantage and disavantage of Probability Based Methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**:  \n",
    "+ Easier for programming, more flexible.\n",
    "+ Can deal with unseen word sequences.\n",
    "\n",
    "**Disadvantages**:  \n",
    "+ Highly dependent on the quality of corpus. The model will be a tragedy if the corpus used is bad.\n",
    "+ The probability of a sentence has negative correlation to the length of it, which isn't always true.   \n",
    "And the probability would be approximately 0 when sentences are long, it would be hard to tell which sentence is more likely to be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How to solve *OOV* problem?\n",
    "\n",
    "If some words are not in our dictionary or corpus. When we using language model, we need to overcome this `out-of-vocabulary`(OOV) problems. There are so many intelligent man to solve this probelm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: How did you solve this problem in your programming task?**  \n",
    "We set the probability of OOV words as $1 / total \\_ count \\_of\\_n\\_grams$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: Read about the 'Good-Turing Estimator', can explain the main points about this method, and may implement this method in your programming task**  \n",
    "Good-Turing frequency estimation is a statistical technique for estimating the probability of hitherto unseen species, given a set of past observations of objects from different species. Considering our language model, it can be used to solve OOV problem mentioned above. In fact, it is one of the commonly used smoothing methods.  \n",
    "**Main point**: reallocate the probability mass of n-grams that occur r + 1 times in the training data to the n-grams that occur r times. In particular, reallocate the probability mass of n-grams that were seen once to the n-grams that were never seen. In mathematical terms:\n",
    "$$\n",
    "P_r = \\frac{(r+1)S(N_{r+1})}{NS(N_r)}\n",
    "$$\n",
    "where $S()$ is the smoothed frequency. for small $r$, $S(N_r) = N_r$, for large $r$, $log(N_r) = a + b \\times log(r).$\n",
    "\n",
    "Reference: \n",
    "+ https://www.wikiwand.com/en/Good%E2%80%93Turing_frequency_estimation\n",
    "+ https://github.com/Computing-Intelligence/References/blob/master/NLP/Natural-Language-Processing.pdf, Page-37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_smoothed(r, slope, intercept):\n",
    "    \"\"\"Returns moothed frequency of frequencies vector.\"\"\"\n",
    "    return np.exp(intercept + slope * np.log(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_turing_estimator(words):\n",
    "    \"\"\"Find simple linear regression parameters.\"\"\"\n",
    "    # frequencies vector\n",
    "    words_count = Counter(words)\n",
    "    # frequency of frequencies vector\n",
    "    count_freq = sorted(Counter(words_count.values()).items())\n",
    "    # simple linear regression, estimate the slope and intercept\n",
    "    r, nr = map(list, zip(*count_freq))\n",
    "    mu_r, mu_nr = sum(np.log(r)) / len(r), sum(np.log(nr)) / len(nr)\n",
    "    slope = sum([(np.log(r[i]) - mu_r) * (np.log(nr[i]) - mu_nr) for i in range(len(r))]) / \\\n",
    "            sum([(np.log(r[i]) - mu_r) ** 2 for i in range(len(r))])\n",
    "    intercept = mu_nr - slope * mu_r\n",
    "        \n",
    "    return intercept, slope   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_turing_prob(word):\n",
    "    \"\"\"Calculate the good turing probability of a single word.\"\"\"\n",
    "    global slope, intercept\n",
    "    global words_count, words_total\n",
    "    r = words_count.get(word, 0)\n",
    "    if r == 0:\n",
    "        nr_next = log_smoothed(r+1, slope, intercept)\n",
    "        return (r + 1) * nr_next / words_total\n",
    "    else:\n",
    "        return r / words_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count, words_total = get_n_gram_counts(VOCABULARY, 1)\n",
    "slope, intercept = good_turing_estimator(VOCABULARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004094423562529852"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word exists\n",
    "get_good_turing_prob(\"an\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0425769483406945e-08"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unseen word\n",
    "get_good_turing_prob(\"subsittute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
