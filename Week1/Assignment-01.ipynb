{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment-01\n",
    "Jiaying Yao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 & 2\n",
    "#### A rule-based chatbot supports both Chinese and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general utility functions\n",
    "def is_variable(pattern):\n",
    "    \"\"\"Check if the pattern is a single variable\"\"\"\n",
    "    return pattern.startswith('?') and all(a.isalpha() for a in pattern[1:])\n",
    "\n",
    "def is_pattern_segment(pattern):\n",
    "    \"\"\"Check if the pattern is a segment variable\"\"\"\n",
    "    return pattern.startswith('?*') and all(a.isalpha() for a in pattern[2:]) \n",
    "\n",
    "def pat_to_dict(patterns):\n",
    "    \"\"\"Transform patterns into dictionarys.\n",
    "    Args:\n",
    "        patterns: List[Tuple]\n",
    "    Returns:\n",
    "        Dict: key is the variable, value is the corresponding segments.\n",
    "    \"\"\"\n",
    "    return {k: ' '.join(v) if isinstance(v, list) else v for k, v in patterns}\n",
    "\n",
    "def substitute(rule, parsed_rules):\n",
    "    \"\"\"Substitute the variables using `parsed_rules`.\n",
    "    Args:\n",
    "        rule: str, containing variables\n",
    "        parsed_rules: Dict\n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    if not rule: return []\n",
    "    return [parsed_rules.get(rule[0], rule[0])] + substitute(rule[1:], parsed_rules)\n",
    "\n",
    "# special utility functions for Chinese\n",
    "from zhon.hanzi import punctuation\n",
    "def is_chinese(uchar):\n",
    "    \"\"\"Check if an unicode is a Chinese character\"\"\"\n",
    "    if (uchar >= u'\\u4e00' and uchar<=u'\\u9fff') or (uchar in punctuation):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def tokenize_sentence(s):\n",
    "    \"\"\"Cut a Chinese sentence into words separated by single space.\"\"\"\n",
    "    s_cut = \" \".join(jieba.cut(s))\n",
    "    return concat_wildcard(s_cut)\n",
    "\n",
    "def concat_wildcard(s):\n",
    "    \"\"\"Remove the whitespace between wildcard characters after word segmentation.\"\"\"\n",
    "    pattern_seg = r\"\\? \\* \"\n",
    "    pattern_whole = r\"\\? \"\n",
    "    return re.sub(pattern_whole, \"?\", re.sub(pattern_seg, \"?*\", s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segment Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail = [False]\n",
    "def pat_match_with_seg(pattern, saying):\n",
    "    \n",
    "    if not pattern:\n",
    "        if not saying:\n",
    "            return []\n",
    "        else:\n",
    "            return fail\n",
    "    elif not all(is_pattern_segment(pat) for pat in pattern) and not saying:\n",
    "        return fail\n",
    "    \n",
    "    pat = pattern[0]\n",
    "    \n",
    "    if is_variable(pat):\n",
    "        return [(pat, saying[0])] + pat_match_with_seg(pattern[1:], saying[1:])\n",
    "    elif is_pattern_segment(pat):\n",
    "        match, index = segment_match(pattern, saying)\n",
    "        if index == -1:\n",
    "            return fail\n",
    "        else:            \n",
    "            return [match] + pat_match_with_seg(pattern[1:], saying[index:])\n",
    "    elif pat == saying[0]:\n",
    "        return pat_match_with_seg(pattern[1:], saying[1:])\n",
    "    else:\n",
    "        return fail # fail\n",
    "    \n",
    "def segment_match(pattern, saying):\n",
    "    \"\"\"Find the maximum segment in `saying` that matches pattern.\"\"\"\n",
    "    seg_pat, rest = pattern[0], pattern[1:]\n",
    "    seg_pat = seg_pat.replace('?*', '?')\n",
    "    \n",
    "    if not rest: return (seg_pat, saying), len(saying)    \n",
    "    \n",
    "    for i, token in enumerate(saying):\n",
    "        if rest[0] == token:\n",
    "            return (seg_pat, saying[:i]), i\n",
    "    \n",
    "    return (), -1 # \n",
    "\n",
    "def is_match(rest, saying):\n",
    "    if not rest and not saying:\n",
    "        return True\n",
    "    # what's the point of this condition? Fuzzy matching?\n",
    "    if not all(a.isalpha() for a in rest[0]):\n",
    "        return True\n",
    "    if rest[0] != saying[0]:\n",
    "        return False\n",
    "    return is_match(rest[1:], saying[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_en(saying, rules):\n",
    "    \"\"\"English chatbot\n",
    "    Input: \n",
    "        saying: str\n",
    "        rules: dict, if not specified, the response\n",
    "               would always be \"Thanks!\"\n",
    "    Output: str\n",
    "    \"\"\"\n",
    "    for pat in rules:\n",
    "        parsed_pattern = pat_match_with_seg(pat.split(), saying.split())\n",
    "        if False not in parsed_pattern:\n",
    "            ans_pattern = random.choice(rules[pat])\n",
    "            dic = pat_to_dict(parsed_pattern)\n",
    "            return \" \".join(substitute(ans_pattern.split(), dic))\n",
    "    else:\n",
    "        return \"Sorry, I don't understand what you are talking about!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_ch(saying, rules):\n",
    "    \"\"\"\"Chinese chatbot\n",
    "    Args: \n",
    "        saying: str\n",
    "        rules: dict, if not specified, the response\n",
    "               would always be \"Thanks!\"\n",
    "    Returns: \n",
    "        str\n",
    "    \"\"\"\n",
    "    # tokenize saying\n",
    "    saying_token = tokenize_sentence(saying).split()\n",
    "    for pat in rules:\n",
    "        pat_token = tokenize_sentence(pat).split()\n",
    "        parsed_pattern = pat_match_with_seg(pat_token, saying_token)\n",
    "        if False not in parsed_pattern:\n",
    "            ans_pattern = random.choice(rules[pat])\n",
    "            ans_pattern_token = tokenize_sentence(ans_pattern).split()\n",
    "            dic = pat_to_dict(parsed_pattern)\n",
    "            return \"\".join(substitute(ans_pattern_token, dic))\n",
    "    else:\n",
    "        return \"抱歉，我不明白你在说什么。\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(saying, rules={\"?*x\": [\"Thanks!\"]}, lang='ch'):\n",
    "    \"\"\"Interface for a chatbot supporting both English and Chinese.\"\"\"\n",
    "    if lang == 'ch':\n",
    "        return get_response_ch(saying, rules=rules)\n",
    "    elif lang == 'en':\n",
    "        return get_response_en(saying, rules=rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = {\n",
    "    '?*x hello ?*y': ['How do you do', 'Please state your problem'],\n",
    "    '?*x I want ?*y': ['what would it mean if you got ?y', 'Why do you want ?y', 'Suppose you got ?y soon'],\n",
    "    '?*x if ?*y': ['Do you really think its likely that ?y', 'Do you wish that ?y', 'What do you think about ?y', 'Really-- if ?y'],\n",
    "    '?*x no ?*y': ['why not?', 'You are being a negative', 'Are you saying \\'No\\' just to be negative?'],\n",
    "    '?*x I was ?*y': ['Were you really', 'Perhaps I already knew you were ?y', 'Why do you tell me you were ?y now?'],\n",
    "    '?*x I feel ?*y': ['Do you often feel ?y ?', 'What other feelings do you have?'],\n",
    "    '?*x你好?*y': ['你好呀', '请告诉我你的问题'],\n",
    "    '?*x我想?*y': ['你觉得?y有什么意义呢？', '为什么你想?y', '你可以想想你很快就可以?y了'],\n",
    "    '?*x我想要?*y': ['?x想问你，你觉得?y有什么意义呢？', '为什么你想?y', '?x觉得，你可以想想你很快就可以有?y了', '你看?x像?y不', '我看你就像?y'],\n",
    "    '?*x喜欢?*y': ['喜欢?y的哪里？', '?y有什么好的呢？', '你想要?y吗？'],\n",
    "    '?*x讨厌?*y': ['?y怎么会那么讨厌呢？', '讨厌?y的哪里？', '?y有什么不好呢？', '你不想要?y吗？'],\n",
    "    '?*xAI?*y': ['你为什么要提AI的事情？', '你为什么觉得AI要解决你的问题？'],\n",
    "    '?*x机器人?*y': ['你为什么要提机器人的事情？', '你为什么觉得机器人要解决你的问题？'],\n",
    "    '?*x对不起?*y': ['不用道歉', '你为什么觉得你需要道歉呢？'],\n",
    "    '?*x我记得?*y': ['你经常会想起这个吗？', '除了?y你还会想起什么吗？', '你为什么和我提起?y'],\n",
    "    '?*x如果?*y': ['你真的觉得?y会发生吗？', '你希望?y吗？', '真的吗？如果?y的话', '关于?y你怎么想？'],\n",
    "    '?*x我?*z梦见?*y':['真的吗？ --- ?y', '你在醒着的时候，以前想象过?y吗？', '你以前梦见过?y吗'],\n",
    "    '?*x妈妈?*y': ['你家里除了?y还有谁？', '嗯嗯，多说一点和你家里有关系的', '她对你影响很大吗？'],\n",
    "    '?*x爸爸?*y': ['你家里除了?y还有谁？', '嗯嗯，多说一点和你家里有关系的', '他对你影响很大吗？', '每当你想起你爸爸的时候，你还会想起其他的吗？'],\n",
    "    '?*x我愿意?*y': ['我可以帮你?y吗？', '你可以解释一下，为什么想?y'],\n",
    "    '?*x我很难过，因为?*y': ['我听到你这么说，也很难过', '?y不应该让你这么难过的'],\n",
    "    '?*x难过?*y': ['我听到你这么说，也很难过',\n",
    "                 '不应该让你这么难过的，你觉得你拥有什么，就会不难过？',\n",
    "                 '你觉得事情变成什么样，你就不难过了？'],\n",
    "    '?*x就像?*y': ['你觉得?x和?y有什么相似性？', '?x和?y真的有关系吗？', '怎么说？'],\n",
    "    '?*x和?*y都?*z': ['你觉得?z有什么问题吗？', '?z会对你有什么影响呢？'],\n",
    "    '?*x和?*y一样?*z': ['你觉得?z有什么问题吗？', '?z会对你有什么影响呢？'],\n",
    "    '?*x我是?*y': ['真的吗？', '?x想告诉你，或许我早就知道你是?y', '你为什么现在才告诉我你是?y'],\n",
    "    '?*x我是?*y吗': ['如果你是?y会怎么样呢？', '你觉得你是?y吗', '如果你是?y，那一位着什么？'],\n",
    "    '?*x你是?*y吗':  ['你为什么会对我是不是?y感兴趣？', '那你希望我是?y吗', '你要是喜欢，我就会是?y'],\n",
    "    '?*x你是?*y' : ['为什么你觉得我是?y'],\n",
    "    '?*x因为?*y' : ['?y是真正的原因吗？', '你觉得会有其他原因吗？'],\n",
    "    '?*x我不能?*y': ['你或许现在就能?y', '如果你能?y,会怎样呢？'],\n",
    "    '?*x我觉得?*y': ['你经常这样感觉吗？', '除了到这个，你还有什么其他的感觉吗？'],\n",
    "    '?*x我?*y你?*z': ['其实很有可能我们互相?y'],\n",
    "    '?*x你为什么不?*y': ['你自己为什么不?y', '你觉得我不会?y', '等我心情好了，我就?y'],\n",
    "    '?*x好的?*y': ['好的', '你是一个很正能量的人'],\n",
    "    '?*x嗯嗯?*y': ['好的', '你是一个很正能量的人'],\n",
    "    '?*x不嘛?*y': ['为什么不？', '你有一点负能量', '你说 不，是想表达不想的意思吗？'],\n",
    "    '?*x不要?*y': ['为什么不？', '你有一点负能量', '你说 不，是想表达不想的意思吗？'],\n",
    "    '?*x有些人?*y': ['具体是哪些人呢？'],\n",
    "    '?*x有的人?*y': ['具体是哪些人呢？'],\n",
    "    '?*x某些人?*y': ['具体是哪些人呢？'],\n",
    "    '?*x每个人?*y': ['我确定不是人人都是', '你能想到一点特殊情况吗？', '例如谁？', '你看到的其实只是一小部分人'],\n",
    "    '?*x所有人?*y': ['我确定不是人人都是', '你能想到一点特殊情况吗？', '例如谁？', '你看到的其实只是一小部分人'],\n",
    "    '?*x总是?*y': ['你能想到一些其他情况吗？', '例如什么时候？', '你具体是说哪一次？', '真的---总是吗？'],\n",
    "    '?*x一直?*y': ['你能想到一些其他情况吗？', '例如什么时候？', '你具体是说哪一次？', '真的---总是吗？'],\n",
    "    '?*x或许?*y': ['你看起来不太确定'],\n",
    "    '?*x可能?*y': ['你看起来不太确定'],\n",
    "    '?*x他们是?*y吗？': ['你觉得他们可能不是?y？'],\n",
    "    # '?*x': ['很有趣', '请继续', '我不太确定我很理解你说的, 能稍微详细解释一下吗?']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/vx/ldqqvh1x50b66d17k1rynyn00000gn/T/jieba.cache\n",
      "Loading model cost 0.591 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请告诉我你的问题\n",
      "想问你，你觉得娃娃有什么意义呢？\n",
      "你为什么现在才告诉我你是人 吗\n",
      "你觉得吗有什么问题吗？\n",
      "Please state your problem\n",
      "Do you often feel frustrated ?\n",
      "\n",
      "抱歉，我不明白你在说什么。\n",
      "Sorry, I don't understand what you are talking about!\n",
      "\n",
      "Thanks!\n"
     ]
    }
   ],
   "source": [
    "print(get_response(\"你好\", rules))\n",
    "print(get_response(\"我想要娃娃\", rules))\n",
    "print(get_response(\"我是人吗\", rules))\n",
    "print(get_response(\"我和你一样吗\", rules))\n",
    "print(get_response(\"hello\", rules, lang='en'))\n",
    "print(get_response(\"I feel frustrated\", rules, lang='en'))\n",
    "\n",
    "print()\n",
    "# rule not exist\n",
    "print(get_response(\"再见\", rules))\n",
    "print(get_response(\"I need an   iPhone\", rules, lang='en'))\n",
    "\n",
    "print()\n",
    "# default rule\n",
    "print(get_response(\"excuse me?\", lang='en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 这样的程序有什么优点？有什么缺点？你有什么可以改进的方法吗？ \n",
    "    * Advantages：\n",
    "       * Rule-based, guaranteed to give meaningful answers. \n",
    "       * Simple to prototype, don't have \"cold start\" problem, easy to add/delete rules.\n",
    "       * Such chatbots can have buttons, carousels and other interactive functions, not restricted to text     interactions. \n",
    "    * Disadvantages:\n",
    "       * Can only handle predefined quetion patterns, can't learn from interactions by itself, not flexible.\n",
    "       * When the application scenario is complicated, the rules can be difficult, if not impossible, to design.\n",
    "       * For this specific implementation, it's not efficient, need to search the entire rule space for pattern-matching. \n",
    "       * For the same question, it randomly chooese an answer which may not be the most appropriate one.\n",
    "    * Improvements:\n",
    "       * Use a syntax tree to store the rules, extract keywords from saying, speed up pattern-matching and search.\n",
    "   \n",
    "   \n",
    "2. 什么是数据驱动？数据驱动在这个程序里如何体现？  \n",
    "Data driven is a way of thinking, it refers to a process or activity that is spurred on by data, as opposed to being driven by mere intuition or personal experience. From the perspective of programming, data-driven is about writing as little fixed code as possible. Take our rule-based chatbot for example, in general, it does not follow the principle of data-driven because all the rules are predefined. But if we adjust the rules based on interactions with users, the data, then this process can be considered as data-driven.\n",
    "\n",
    "\n",
    "3. 数据驱动与 AI 的关系是什么？   \n",
    "First we need to know what is AI? There are a lot of different definitions of AI, in [Merriam-Webster Dictionary](https://www.merriam-webster.com/dictionary/artificial%20intelligence), AI is defined as the capability of a machine to imitate intelligent human behavior. Generally, there are two kinds of AI, model-driven AI and data-driven AI. \n",
    "Model-driven AI attempts to capture knowledge and derive decisions through explicit representation and rules. The data-driven way focusses on building a system that can identify what is the right answer based on having “seen” a large number of examples of question / answer pairs and “training” it to get to the right answer. Machine learning belongs to data-driven AI and data-driven AI has become the trend of AI due to the rise of Big Data and computional power. \n",
    "\n",
    "Reference: [Building AI software: Data-driven vs model-driven AI and why we need an AI-specific software development paradigm](https://hackernoon.com/building-ai-software-data-driven-vs-model-driven-ai-and-why-we-need-an-ai-specific-software-640f74aaf78f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data100",
   "language": "python",
   "name": "data100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
